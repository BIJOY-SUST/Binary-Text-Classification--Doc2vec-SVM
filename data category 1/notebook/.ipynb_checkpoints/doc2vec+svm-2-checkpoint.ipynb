{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/.local/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "import gensim\n",
    "import collections\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "__REPL__ = \".\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "__re_collapse_spaces__ = re.compile(\"\\s+\")\n",
    "__re_remove_special_chars__ = re.compile(\"[;:\\'\\\"\\*/\\),\\(\\|\\s]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_spaces(s):\n",
    "    return __re_collapse_spaces__.sub(\" \", s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    s = str(s).replace(\"'s\",\" \")\n",
    "    s = s.replace(\"-\", \" \").replace(\"\\\\\",\" \")\n",
    "    s = __re_remove_special_chars__.sub(\" \",s).strip()\n",
    "    return collapse_spaces(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(txt):\n",
    "    txt = txt.replace(\"</li><li>\", __REPL__).replace(\"<li>\", __REPL__).replace(\"</li>\", __REPL__)\n",
    "    txt = txt.replace(\"<br>\", __REPL__)\n",
    "    txt = txt.replace(\"<br/>\", __REPL__)\n",
    "    txt = txt.replace(\"<br />\", __REPL__)\n",
    "    txt = txt.replace(\"<p>\",  __REPL__)\n",
    "    txt = txt.replace(\"<p/>\",  __REPL__)\n",
    "    txt = txt.replace(\"<p />\",  __REPL__)\n",
    "    txt = txt.replace(\"</p>\", __REPL__)\n",
    "    txt = txt.replace(\". .\",  __REPL__)\n",
    "    txt = txt.replace(\"&nbsp;\", \" \")\n",
    "    while \"..\" in txt:\n",
    "        txt = txt.replace(\"..\", \". \")\n",
    "    while \"  \" in txt:\n",
    "        txt = txt.replace(\"  \", \" \")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', strip_non_ascii(element)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "    bs = BeautifulSoup(html,\"html5lib\")\n",
    "    texts = bs.findAll(text=True)\n",
    "    visible_texts = filter(if_visible, texts)\n",
    "    return __REPL__.join(visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_html(html):\n",
    "    txt = get_text(pre_process_text(html))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(txt):\n",
    "    txt = strip_non_ascii(txt)\n",
    "    sents = map(clean_str,sent_tokenize(txt))\n",
    "    return filter(lambda s: len(s.strip()) > 5, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntpath.basename(\"a/b/c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Files\n",
    "\n",
    "def data_cleaning_process(documents_folder=\"\", processed_documents_folder=\"\", parse_html=True, minimum_file_size = 0):\n",
    "    print(documents_folder)\n",
    "    print(processed_documents_folder)\n",
    "    start = time.time()\n",
    "    files = os.listdir(processed_documents_folder)\n",
    "\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            os.remove(processed_documents_folder + file_path)\n",
    "\n",
    "    files = os.listdir(documents_folder)\n",
    "    for i, fpath in enumerate(files):\n",
    "        with open(documents_folder + fpath) as f:\n",
    "            contents = f.read()\n",
    "            if len(contents) < minimum_file_size:\n",
    "                continue\n",
    "            if parse_html:\n",
    "                contents = parsing_html(contents)\n",
    "                if len(contents) < minimum_file_size:\n",
    "                    continue\n",
    "            sents = split_into_sentences(contents)\n",
    "            doc = \"\\n\".join(sents)\n",
    "            file_name = get_file_name(fpath)\n",
    "            fout_name = processed_documents_folder + \"/\" + file_name.split(\".\")[0] + \"_proc.txt\"\n",
    "            with open(fout_name, \"w+\") as fout:\n",
    "                fout.write(doc)\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(\"%i documents processsed\" % i)\n",
    "    end = time.time()\n",
    "    print(\"Loading and processing documents took %s seconds\" % str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is cleaning...\n",
      "Train data is cleaning...\n",
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/training-data/\n",
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-training-data/\n",
      "Loading and processing documents took 0.15104246139526367 seconds\n",
      "Test data is cleaning...\n",
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/test-data/\n",
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-test-data/\n",
      "Loading and processing documents took 0.05474448204040527 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Data set is cleaning...\")\n",
    "\n",
    "print(\"Train data is cleaning...\")\n",
    "data_cleaning_process(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/training-data/\", \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-training-data/\")\n",
    "\n",
    "print(\"Test data is cleaning...\")\n",
    "data_cleaning_process(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/test-data/\", \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-test-data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            f = open(doc.name,'r')\n",
    "            lines = f.read()\n",
    "            yield gensim.models.doc2vec.TaggedDocument(words=nltk.word_tokenize(lines),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_run():\n",
    "    docLabels1 = [f for f in listdir(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-training-data/\") if f.endswith('.txt')]\n",
    "    data = []\n",
    "    for doc in docLabels1:\n",
    "        f = open(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-training-data/\" + doc, 'r')\n",
    "        data.append(f)\n",
    "        f.close()\n",
    "        \n",
    "    docLabels2 = [f for f in listdir(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-test-data/\") if f.endswith('.txt')]\n",
    "#     data = []\n",
    "    for doc in docLabels2:\n",
    "        f = open(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-test-data/\" + doc, 'r')\n",
    "        data.append(f)\n",
    "        f.close()\n",
    "    \n",
    "    docLabelsAll=docLabels1+docLabels2\n",
    "    print(len(docLabelsAll))\n",
    "    it = DocIterator(data, docLabelsAll)\n",
    "    model = gensim.models.Doc2Vec(size=300, window=8, min_count=5, workers=4, alpha=0.025, min_alpha=0.025, dm = 0, dbow_words=1) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    print (\"-----Doc2vec Training Started----\")\n",
    "    for epoch in range(10):\n",
    "        print(\"epoch is: \" + str(epoch))\n",
    "        model.train(it,total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.0025   # decrease the learning rate\n",
    "        model.min_alpha = model.alpha   # fix the learning rate, no deca\n",
    "\n",
    "    model.save(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/model/doc2vec.model\")\n",
    "    print(\"Doc2vec model saved into the disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2vec is training. Please wait...\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Doc2vec Training Started----\n",
      "epoch is: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is: 1\n",
      "epoch is: 2\n",
      "epoch is: 3\n",
      "epoch is: 4\n",
      "epoch is: 5\n",
      "epoch is: 6\n",
      "epoch is: 7\n",
      "epoch is: 8\n",
      "epoch is: 9\n",
      "Doc2vec model saved into the disk\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc2vec is training. Please wait...\")\n",
    "doc2vec_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_run(training_folder, label_file):\n",
    "    doc2vec_model = gensim.models.Doc2Vec.load(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/model/doc2vec.model\")\n",
    "    file_names = os.listdir(training_folder)\n",
    "    label_dict = {}\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(label_file, \"r\") as label_file:\n",
    "        lines = label_file.readlines()\n",
    "        for line in lines:\n",
    "            key, value = line.split(\" \")\n",
    "            label_dict[key] = str(value).replace(\"\\r\", \"\").replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    labels = [item for item, count in collections.Counter(list(label_dict.values())).items() if count >= 1]\n",
    "    for file_name in file_names:\n",
    "        with open(training_folder + file_name, 'r') as f:\n",
    "            lines = f.read()\n",
    "        words = nltk.word_tokenize(lines)\n",
    "        X.append(doc2vec_model.infer_vector(words))\n",
    "        if label_dict[file_name.split(\"_\")[0]] == labels[1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(-1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is building...\n",
      "Train data is building...\n",
      "Test data is building...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data set is building...\")\n",
    "\n",
    "print(\"Train data is building...\")\n",
    "X_train, Y_train = build_dataset_run(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-training-data/\", \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/training-class\")\n",
    "\n",
    "print(\"Test data is building...\")\n",
    "X_test, Y_test = build_dataset_run(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/processed-test-data/\", \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/dataset/test-class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_run(X,Y):\n",
    "    print(\"SVM training started...\")\n",
    "    clf = SVC()\n",
    "    svm_model = clf.fit(X, Y)\n",
    "    joblib.dump(svm_model, '/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/model/svm.pkl')\n",
    "    print(\"SVM model saved into the disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM training started...\n",
      "SVM model saved into the disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_run(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/model/svm.pkl')\n",
    "\n",
    "y_train_pred = loaded_model.predict(X_train[0:])\n",
    "\n",
    "y_test_pred = loaded_model.predict(X_test[0:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, y_pred):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for index, element_y in enumerate(y):\n",
    "        if element_y == 1:\n",
    "            if y_pred[index] == -1:\n",
    "                fn+= 1\n",
    "            else:\n",
    "                tp+= 1\n",
    "        else:\n",
    "            if y_pred[index] == -1:\n",
    "                tn+= 1\n",
    "            else:\n",
    "                fp+= 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train Results ----\n",
      "True Positive = 0\n",
      "False Positive  = 0\n",
      "True Negative = 35\n",
      "False Negative = 15\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Train Results ----\")\n",
    "tp, tn, fp, fn = metrics(Y_train, y_train_pred)\n",
    "print (\"True Positive = \" + str(tp))\n",
    "print (\"False Positive  = \" + str(fp))\n",
    "print (\"True Negative = \" + str(tn))\n",
    "print (\"False Negative = \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive = 0\n",
      "False Positive  = 0\n",
      "True Negative = 19\n",
      "False Negative = 1\n"
     ]
    }
   ],
   "source": [
    "tp, tn, fp, fn = metrics(Y_test, y_test_pred)\n",
    "print (\"True Positive = \" + str(tp))\n",
    "print (\"False Positive  = \" + str(fp))\n",
    "print (\"True Negative = \" + str(tn))\n",
    "print (\"False Negative = \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.7\n",
      "Test Accuracy :  0.95\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(Y_train, y_train_pred)\n",
    "print(\"Train Accuracy : \",train_accuracy)\n",
    "\n",
    "test_accuracy = accuracy_score(Y_test, y_test_pred)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score :  0.4117647058823529\n",
      "Test F1 Score :  0.48717948717948717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_f1 = f1_score(Y_train, y_train_pred,average='macro')\n",
    "print(\"Train F1 Score : \",train_f1)\n",
    "\n",
    "test_f1 = f1_score(Y_test, y_test_pred,average='macro')\n",
    "print(\"Test F1 Score : \",test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
