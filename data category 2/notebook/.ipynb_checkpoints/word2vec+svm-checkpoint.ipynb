{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/.local/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "import gensim\n",
    "import collections\n",
    "import re, time , ntpath\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents took 0.004503488540649414 seconds\n"
     ]
    }
   ],
   "source": [
    "# data - process\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "OUTCOME = \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/dataset/outcome\"\n",
    "label_dict = {}\n",
    "with open(OUTCOME, \"r\") as label_file:\n",
    "    lines = label_file.readlines()\n",
    "    for line in lines:\n",
    "        key, value = line.split(\" \")\n",
    "        label_dict[key] = str(value).replace(\"\\r\", \"\").replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "PATH = \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/dataset/data/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "files = os.listdir(PATH)\n",
    "for i, fpath in enumerate(files):\n",
    "    with open(PATH + fpath) as f:\n",
    "        contents = f.read()\n",
    "        if len(contents) < 0:\n",
    "            continue\n",
    "\n",
    "        file_name = get_file_name(fpath)\n",
    "        \n",
    "        temp = []\n",
    "        temp.append(file_name)\n",
    "        temp.append(label_dict[file_name])\n",
    "        temp.append(contents)\n",
    "        \n",
    "        \n",
    "        data.append(temp)\n",
    "        \n",
    "        \n",
    "end = time.time()\n",
    "print(\"Loading documents took %s seconds\" % str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>outcome</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-0016</td>\n",
       "      <td>true</td>\n",
       "      <td>Communist protesters greet Hillary Clinton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training-0027</td>\n",
       "      <td>true</td>\n",
       "      <td>BRUSSELS: France and Germany will have to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training-0019</td>\n",
       "      <td>true</td>\n",
       "      <td>French foreign minister meets Algerian pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-0013</td>\n",
       "      <td>true</td>\n",
       "      <td>(ALPE D'HUZE) Overall: 1. Miguel Indurain (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training-0037</td>\n",
       "      <td>false</td>\n",
       "      <td>UNITA Expected Back in Luanda, Battle Front...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>training-0016</td>\n",
       "      <td>true</td>\n",
       "      <td>A threat to shell a Canadian peacekeeping c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>training-0042</td>\n",
       "      <td>true</td>\n",
       "      <td>Car bomb explodes in northern Spain \\n\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>training-0010</td>\n",
       "      <td>true</td>\n",
       "      <td>Taliban Agree on Proposal by U.S. Envoy: Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>training-0038</td>\n",
       "      <td>true</td>\n",
       "      <td>Falun Gong: Anti-Humanity, Denounces People...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>training-0004</td>\n",
       "      <td>false</td>\n",
       "      <td>News Analysis: Thai PM-Apparent Faces Tough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fileName outcome                                               data\n",
       "0       test-0016    true     Communist protesters greet Hillary Clinton ...\n",
       "1   training-0027    true     BRUSSELS: France and Germany will have to h...\n",
       "2   training-0019    true     French foreign minister meets Algerian pres...\n",
       "3       test-0013    true     (ALPE D'HUZE) Overall: 1. Miguel Indurain (...\n",
       "4   training-0037   false     UNITA Expected Back in Luanda, Battle Front...\n",
       "..            ...     ...                                                ...\n",
       "65  training-0016    true     A threat to shell a Canadian peacekeeping c...\n",
       "66  training-0042    true     Car bomb explodes in northern Spain \\n\\n   ...\n",
       "67  training-0010    true     Taliban Agree on Proposal by U.S. Envoy: Re...\n",
       "68  training-0038    true     Falun Gong: Anti-Humanity, Denounces People...\n",
       "69  training-0004   false     News Analysis: Thai PM-Apparent Faces Tough...\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.DataFrame(data, columns=['fileName','outcome','data'])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.data = all_data.data.str.replace('\\n',' ')\n",
    "all_data.data = all_data.data.str.replace('[^\\w\\s]',' ')\n",
    "all_data.data = all_data.data.str.replace('_',' ')\n",
    "all_data.data = all_data.data.str.replace('\\n',' ')\n",
    "all_data.data = all_data.data.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = all_data\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.outcome, random_state=0, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data,model_path):\n",
    "    model = Word2Vec(data, size=300, sg=1, window=5, min_count=1, workers=4, iter=50)\n",
    "    model.save(model_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "train_model = model(x_train,'/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/train_model.model')\n",
    "test_model = model(x_test,'/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/test_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_vec(size,sent,model):\n",
    "    vec = np.zeros(size).reshape(1,size)\n",
    "    count = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec += model[word].reshape(1,size)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_vec(x_train,x_test,train_model,test_model):\n",
    "    train_vec = np.concatenate([get_sent_vec(300,sent,train_model) for sent in x_train])\n",
    "    test_vec = np.concatenate([get_sent_vec(300,sent,test_model) for sent in x_test])\n",
    "    np.save('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/train_vec.npy',train_vec)\n",
    "    np.save('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/test_vec.npy',test_vec)\n",
    "    return train_vec,test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# make-vector\n",
    "train_vec,test_vec = get_train_vec(x_train,x_test,train_model,test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = np.load('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/train_vec.npy')\n",
    "test_vec = np.load('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/test_vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_tran(train_vec,y_train,test_vec,y_test):\n",
    "    clf = SVC(kernel='rbf',verbose=True)\n",
    "    clf.fit(train_vec,y_train)\n",
    "    joblib.dump(clf,'/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/svm_model.pkl',compress=3)\n",
    "    print(clf.score(test_vec,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_tran(train_vec,y_train,test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/model/svm_model.pkl')\n",
    "\n",
    "y_train_pred = loaded_model.predict(train_vec)\n",
    "\n",
    "y_test_pred = loaded_model.predict(test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, y_pred):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for index, element_y in enumerate(y):\n",
    "        if element_y == 1:\n",
    "            if y_pred[index] == -1:\n",
    "                fn+= 1\n",
    "            else:\n",
    "                tp+= 1\n",
    "        else:\n",
    "            if y_pred[index] == -1:\n",
    "                tn+= 1\n",
    "            else:\n",
    "                fp+= 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train Results ----\n",
      "True Positive = 0\n",
      "False Positive  = 49\n",
      "True Negative = 0\n",
      "False Negative = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Train Results ----\")\n",
    "tp, tn, fp, fn = metrics(y_train, y_train_pred)\n",
    "print (\"True Positive = \" + str(tp))\n",
    "print (\"False Positive  = \" + str(fp))\n",
    "print (\"True Negative = \" + str(tn))\n",
    "print (\"False Negative = \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive = 0\n",
      "False Positive  = 21\n",
      "True Negative = 0\n",
      "False Negative = 0\n"
     ]
    }
   ],
   "source": [
    "tp, tn, fp, fn = metrics(y_test, y_test_pred)\n",
    "print (\"True Positive = \" + str(tp))\n",
    "print (\"False Positive  = \" + str(fp))\n",
    "print (\"True Negative = \" + str(tn))\n",
    "print (\"False Negative = \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.7959183673469388\n",
      "Test Accuracy :  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train Accuracy : \",train_accuracy)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score :  0.4431818181818182\n",
      "Test F1 Score :  0.41666666666666663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred,average='macro')\n",
    "print(\"Train F1 Score : \",train_f1)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_test_pred,average='macro')\n",
    "print(\"Test F1 Score : \",test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
