{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/.local/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "import gensim\n",
    "import collections\n",
    "import re, time , ntpath\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/data category 2/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents took 0.0029420852661132812 seconds\n"
     ]
    }
   ],
   "source": [
    "# data - process\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "OUTCOME = \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/data category 2/dataset/outcome\"\n",
    "label_dict = {}\n",
    "with open(OUTCOME, \"r\") as label_file:\n",
    "    lines = label_file.readlines()\n",
    "    for line in lines:\n",
    "        key, value = line.split(\" \")\n",
    "        label_dict[key] = str(value).replace(\"\\r\", \"\").replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "PATH = \"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/data category 2/dataset/data/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "files = os.listdir(PATH)\n",
    "for i, fpath in enumerate(files):\n",
    "    with open(PATH + fpath) as f:\n",
    "        contents = f.read()\n",
    "        if len(contents) < 0:\n",
    "            continue\n",
    "\n",
    "        file_name = get_file_name(fpath)\n",
    "        \n",
    "        temp = []\n",
    "        temp.append(file_name)\n",
    "        temp.append(label_dict[file_name])\n",
    "        temp.append(contents)\n",
    "        \n",
    "        \n",
    "        data.append(temp)\n",
    "        \n",
    "        \n",
    "end = time.time()\n",
    "print(\"Loading documents took %s seconds\" % str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>outcome</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-0016</td>\n",
       "      <td>true</td>\n",
       "      <td>Communist protesters greet Hillary Clinton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training-0027</td>\n",
       "      <td>true</td>\n",
       "      <td>BRUSSELS: France and Germany will have to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training-0019</td>\n",
       "      <td>true</td>\n",
       "      <td>French foreign minister meets Algerian pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-0013</td>\n",
       "      <td>true</td>\n",
       "      <td>(ALPE D'HUZE) Overall: 1. Miguel Indurain (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training-0037</td>\n",
       "      <td>false</td>\n",
       "      <td>UNITA Expected Back in Luanda, Battle Front...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>training-0016</td>\n",
       "      <td>true</td>\n",
       "      <td>A threat to shell a Canadian peacekeeping c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>training-0042</td>\n",
       "      <td>true</td>\n",
       "      <td>Car bomb explodes in northern Spain \\n\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>training-0010</td>\n",
       "      <td>true</td>\n",
       "      <td>Taliban Agree on Proposal by U.S. Envoy: Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>training-0038</td>\n",
       "      <td>true</td>\n",
       "      <td>Falun Gong: Anti-Humanity, Denounces People...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>training-0004</td>\n",
       "      <td>false</td>\n",
       "      <td>News Analysis: Thai PM-Apparent Faces Tough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fileName outcome                                               data\n",
       "0       test-0016    true     Communist protesters greet Hillary Clinton ...\n",
       "1   training-0027    true     BRUSSELS: France and Germany will have to h...\n",
       "2   training-0019    true     French foreign minister meets Algerian pres...\n",
       "3       test-0013    true     (ALPE D'HUZE) Overall: 1. Miguel Indurain (...\n",
       "4   training-0037   false     UNITA Expected Back in Luanda, Battle Front...\n",
       "..            ...     ...                                                ...\n",
       "65  training-0016    true     A threat to shell a Canadian peacekeeping c...\n",
       "66  training-0042    true     Car bomb explodes in northern Spain \\n\\n   ...\n",
       "67  training-0010    true     Taliban Agree on Proposal by U.S. Envoy: Re...\n",
       "68  training-0038    true     Falun Gong: Anti-Humanity, Denounces People...\n",
       "69  training-0004   false     News Analysis: Thai PM-Apparent Faces Tough...\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.DataFrame(data, columns=['fileName','outcome','data'])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.data = all_data.data.str.replace('\\n',' ')\n",
    "all_data.data = all_data.data.str.replace('[^\\w\\s]',' ')\n",
    "all_data.data = all_data.data.str.replace('_',' ')\n",
    "all_data.data = all_data.data.str.replace('\\n',' ')\n",
    "all_data.data = all_data.data.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the LabeledSentence method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the review.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.LabeledSentence(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "dataset = all_data\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.outcome, random_state=0, test_size=0.3)\n",
    "x_train = label_sentences(x_train, 'Train')\n",
    "x_test = label_sentences(x_test, 'Test')\n",
    "all_data = x_train + x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(corpus):\n",
    "    d2v = doc2vec.Doc2Vec(min_count=1,  # Ignores all words with total frequency lower than this\n",
    "                          window=10,  # The maximum distance between the current and predicted word within a sentence\n",
    "                          vector_size=300,  # Dimensionality of the generated feature vectors\n",
    "                          workers=5,  # Number of worker threads to train the model\n",
    "                          alpha=0.025,  # The initial learning rate\n",
    "                          min_alpha=0.00025,  # Learning rate will linearly drop to min_alpha as training progresses\n",
    "                          dm=0, # dm defines the training algorithm. If dm=1 means ‘distributed memory’ (PV-DM)\n",
    "                                 # and dm =0 means ‘distributed bag of words’ (PV-DBOW)\n",
    "                          dbow_words  = 1,\n",
    "                          dm_mean = 1)  \n",
    "    d2v.build_vocab(corpus)\n",
    "\n",
    "    # 10 epochs take around 10 minutes on my machine (i7), if you have more time/computational power make it 20\n",
    "    for epoch in range(10):\n",
    "        d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.iter)\n",
    "        # shuffle the corpus\n",
    "        random.shuffle(corpus)\n",
    "        # decrease the learning rate\n",
    "        d2v.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        d2v.min_alpha = d2v.alpha\n",
    "\n",
    "    d2v.save(\"/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/doc2vec+svm/data category 2/model/d2v.model\")\n",
    "    return d2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(doc2vec_model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = doc2vec_model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, y_pred):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for index, element_y in enumerate(y):\n",
    "        if element_y == 1:\n",
    "            if y_pred[index] == -1:\n",
    "                fn+= 1\n",
    "            else:\n",
    "                tp+= 1\n",
    "        else:\n",
    "            if y_pred[index] == -1:\n",
    "                tn+= 1\n",
    "            else:\n",
    "                fp+= 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(d2v, training_vectors, training_labels):\n",
    "    train_vectors = get_vectors(d2v, len(training_vectors), 300, 'Train')\n",
    "    model = LinearSVC(penalty='l2')\n",
    "    model.fit(train_vectors, np.array(training_labels))\n",
    "    training_predictions = model.predict(train_vectors)\n",
    "    \n",
    "    print(\"---- Train Results ----\")\n",
    "    tp, tn, fp, fn = metrics(training_labels, training_predictions)\n",
    "    print (\"True Positive = \" + str(tp))\n",
    "    print (\"False Positive  = \" + str(fp))\n",
    "    print (\"True Negative = \" + str(tn))\n",
    "    print (\"False Negative = \" + str(fn))\n",
    "    \n",
    "    train_accuracy = accuracy_score(training_labels, training_predictions)\n",
    "    print(\"Train Accuracy : \",train_accuracy)\n",
    "    \n",
    "    train_f1 = f1_score(training_labels, training_predictions,average='macro')\n",
    "    print(\"Train F1 Score : \",train_f1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(d2v, classifier, testing_vectors, testing_labels):\n",
    "    test_vectors = get_vectors(d2v, len(testing_vectors), 300, 'Test')\n",
    "    testing_predictions = classifier.predict(test_vectors)\n",
    "\n",
    "    tp, tn, fp, fn = metrics(testing_labels, testing_predictions)\n",
    "    print (\"True Positive = \" + str(tp))\n",
    "    print (\"False Positive  = \" + str(fp))\n",
    "    print (\"True Negative = \" + str(tn))\n",
    "    print (\"False Negative = \" + str(fn))\n",
    "\n",
    "    test_accuracy = accuracy_score(testing_labels, testing_predictions)\n",
    "    print(\"Test Accuracy : \",test_accuracy)\n",
    "\n",
    "    test_f1 = f1_score(testing_labels, testing_predictions,average='macro')\n",
    "    print(\"Test F1 Score : \",test_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  app.launch_new_instance()\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train Results ----\n",
      "True Positive = 0\n",
      "False Positive  = 49\n",
      "True Negative = 0\n",
      "False Negative = 0\n",
      "Train Accuracy :  1.0\n",
      "Train F1 Score :  1.0\n",
      "True Positive = 0\n",
      "False Positive  = 21\n",
      "True Negative = 0\n",
      "False Negative = 0\n",
      "Test Accuracy :  0.6190476190476191\n",
      "Test F1 Score :  0.3823529411764707\n"
     ]
    }
   ],
   "source": [
    "d2v_model = train_doc2vec(all_data)\n",
    "classifier = train_classifier(d2v_model, x_train, y_train)\n",
    "test_classifier(d2v_model, classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
